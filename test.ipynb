{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048252e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('/content/drive/MyDrive/loan_data.csv')\n",
    "\n",
    "numeric_features = ['annual_income', 'debt_to_income_ratio', 'credit_score', \n",
    "                    'loan_amount', 'interest_rate']\n",
    "\n",
    "categorical_nominal = ['gender', 'marital_status', 'employment_status', \n",
    "                       'loan_purpose']\n",
    "\n",
    "# Added 'grade_subgrade' here. \n",
    "# Alphabetical sorting (A1, A2, B1...) works perfectly for risk grading.\n",
    "categorical_ordinal = ['education_level', 'grade_subgrade']\n",
    "\n",
    "# --- 2. Define Preprocessing Pipelines ---\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_nom_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "cat_ord_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# --- 3. Combine into a Preprocessor ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numeric_features),\n",
    "        ('cat_nom', cat_nom_pipeline, categorical_nominal),\n",
    "        ('cat_ord', cat_ord_pipeline, categorical_ordinal)\n",
    "    ])\n",
    "\n",
    "# --- 4. Split Data (Corrected Drop) ---\n",
    "# We only drop 'id' and the target. We keep 'grade_subgrade' as a feature.\n",
    "X = df.drop(['id', 'loan_paid_back'], axis=1)\n",
    "y = df['loan_paid_back']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data successfully split!\")\n",
    "print(f\"Training shape: {X_train.shape}\")\n",
    "print(f\"Testing shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da837585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c235d83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# 1. create the Full Pipeline with Model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, \n",
    "                                          max_depth=10, \n",
    "                                          random_state=42, \n",
    "                                          n_jobs=-1))\n",
    "])\n",
    "\n",
    "# 2. Train the Model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict Probabilities (We need probability for AUC, not just class labels)\n",
    "# predict_proba returns [prob_class_0, prob_class_1]\n",
    "y_pred_probs = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 4. Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "print(f\"Overall Model AUC on Test Set: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca05d12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate AUC for specific groups\n",
    "def calculate_subgroup_metrics(X_test, y_test, y_pred_probs, column_name):\n",
    "    # Create a temporary dataframe for analysis to keep raw values\n",
    "    analysis_df = X_test.copy()\n",
    "    analysis_df['target'] = y_test\n",
    "    analysis_df['score'] = y_pred_probs\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Iterate through unique values in the subgroup (e.g., 'PhD', 'Masters')\n",
    "    for group in analysis_df[column_name].unique():\n",
    "        subset = analysis_df[analysis_df[column_name] == group]\n",
    "        \n",
    "        # We need at least one '0' and one '1' in the target to calculate AUC\n",
    "        if len(subset['target'].unique()) > 1:\n",
    "            auc = roc_auc_score(subset['target'], subset['score'])\n",
    "            results[group] = auc\n",
    "        else:\n",
    "            results[group] = \"N/A (Not enough data)\"\n",
    "            \n",
    "    return results\n",
    "\n",
    "# 1. Analyze by Education Level\n",
    "print(\"--- Fairness Report: Education Level ---\")\n",
    "edu_results = calculate_subgroup_metrics(X_test, y_test, y_pred_probs, 'education_level')\n",
    "\n",
    "# Sort and print\n",
    "for level, score in sorted(edu_results.items()):\n",
    "    print(f\"Education: {level:20} | AUC: {score:.4f}\")\n",
    "\n",
    "\n",
    "# 2. Analyze by Loan Purpose (Top 3 vs Bottom 3)\n",
    "print(\"\\n--- Fairness Report: Loan Purpose ---\")\n",
    "purpose_results = calculate_subgroup_metrics(X_test, y_test, y_pred_probs, 'loan_purpose')\n",
    "\n",
    "# Sort by AUC score (descending)\n",
    "sorted_purposes = sorted(\n",
    "    [item for item in purpose_results.items() if isinstance(item[1], float)], \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"Top 3 Performing Purposes (Safest Predictions):\")\n",
    "for p in sorted_purposes[:3]:\n",
    "    print(f\"  {p[0]:20}: {p[1]:4f}\")\n",
    "\n",
    "print(\"\\nBottom 3 Performing Purposes (Hardest to Predict):\")\n",
    "for p in sorted_purposes[-3:]:\n",
    "    print(f\"  {p[0]:20}: {p[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760bfb61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Extract Feature Names from the Preprocessor\n",
    "# This automatically gets names for One-Hot Encoded columns too\n",
    "feature_names = model_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# 2. Get Importance values from the Random Forest\n",
    "importances = model_pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# 3. Create a DataFrame for plotting\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# 4. Sort and Plot Top 10 Features\n",
    "top_10 = feat_imp_df.sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_10, palette='viridis')\n",
    "plt.title('Top 10 Drivers of Loan Default Risk')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e385c95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Retrieve the original IDs for the test set samples\n",
    "# (We use the index of X_test to find the matching IDs in the original df)\n",
    "test_ids = df.loc[X_test.index, 'id']\n",
    "\n",
    "# 2. Create the Results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'predicted_probability': y_pred_probs,  # The probability of class 1 (Paid Back)\n",
    "    'actual_outcome': y_test                # Useful for visual comparison\n",
    "})\n",
    "\n",
    "# 3. Sort by probability (optional, to see highest risk vs safest loans)\n",
    "results_df = results_df.sort_values(by='predicted_probability', ascending=True)\n",
    "\n",
    "# 4. Display the first 10 rows (The highest risk loans)\n",
    "print(\"--- Lowest Probability of Repayment (High Risk) ---\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# 5. Display the last 10 rows (The safest loans)\n",
    "print(\"\\n--- Highest Probability of Repayment (Safe) ---\")\n",
    "print(results_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9bb6e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e638195",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
